# -*- coding: utf-8 -*-
"""xse_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EQy9vTgeEbljYzbaWtFnug2r3L595TOg
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import glob
import os
import pickle as pkl

from google.colab import drive
drive.mount('/content/gdrive')

LANGUAGE = {'asm':0,'ben':1,'guj':2,'hin':3,
            'kan':4,'mal':5,'odi':6,'tel':7}

def prepare_data():
    X_train,y_train = [],[]
    folders = glob.glob('gdrive/MyDrive/MTP/Codework/BNF_Data_LID/train')
    for folder in folders:
        for lang in ['asm','ben','guj','hin','kan','mal','odi','tel']:
            cnt =  0
            y = LANGUAGE[lang]
            y = torch.reshape(torch.tensor(y),[1])
            for f in glob.glob(folder+'/'+lang+'/*.csv'):
                x = pd.read_csv(f)
                x = torch.reshape(torch.tensor(x.values),[-1,80]).float()
                X_train.append(x)
                y_train.append(y)
                cnt+=1
    data = list(zip(X_train,y_train))
    np.random.shuffle(data)
    X_train,y_train = zip(*data)
    return X_train,y_train

class se_encoder(nn.Module):
    def __init__(self,input_size, output_size):
        super(se_encoder,self).__init__()
        
        # attention block 1
        self.afc1 = nn.Linear(input_size, input_size//16)
        self.afc2 = nn.Linear(input_size//16, input_size)
        
        # attention block 2
        self.afc3 = nn.Linear(input_size, input_size//16)
        self.afc4 = nn.Linear(input_size//16, input_size)
        
        # learnable weights
        self.vfc1 = nn.Linear(input_size, input_size)
        self.softmax = nn.Softmax(dim=0)
        
    def forward(self, x):
        # extracting h
        x1 = F.relu(self.afc1(x))
        h = F.relu(self.afc2(x1))
        
        # extracting attention
        a = self.softmax(self.vfc1(h))
        
        # extracting m
        m = (a*h).sum(axis=0)
        #m = torch.bmm(a, h) 
        m1 = F.relu(self.afc3(m))
        s = self.softmax(self.afc4(m1))
        
        # extracted xhat
        xhat = torch.mul(s,x)
        return(xhat)
    
class LSTMNet(torch.nn.Module):
    def __init__(self):
        super(LSTMNet, self).__init__()
        self.lstm1 = nn.LSTM(80, 256,bidirectional=True)
        self.lstm2 = nn.LSTM(2*256, 32,bidirectional=True)
               
        self.fc_ha=nn.Linear(2*32,100) 
        self.fc_1= nn.Linear(100,1)           
        self.sftmax = torch.nn.Softmax(dim=1)

    def forward(self, x):
        x1, _ = self.lstm1(x) 
        x2, _ = self.lstm2(x1)
        ht = x2[-1]
        ht=torch.unsqueeze(ht, 0)        
        ha= torch.tanh(self.fc_ha(ht))
        alp= self.fc_1(ha)
        al= self.sftmax(alp) 
        
       
        T=list(ht.shape)[1]  
        batch_size=list(ht.shape)[0]
        D=list(ht.shape)[2]
        c=torch.bmm(al.view(batch_size, 1, T),ht.view(batch_size,T,D))
        c = torch.squeeze(c,0)        
        return (c)

class MSA_DAT_Net(nn.Module):
    def __init__(self, model1,model2):
        super(MSA_DAT_Net, self).__init__()
        self.model1 = model1
        self.model2 = model2

        self.att1=nn.Linear(2*32,100) 
        self.att2= nn.Linear(100,1)           
        self.bsftmax = torch.nn.Softmax(dim=1)

        self.lang_classifier= nn.Sequential()
        self.lang_classifier.add_module('fc1',nn.Linear(2*32,8,bias=True))     
        
        
    def forward(self, x1,x2):
        u1 = self.model1(x1)
        u2 = self.model2(x2)        
        ht_u = torch.cat((u1,u2), dim=0)  
        ht_u = torch.unsqueeze(ht_u, 0) 
        ha_u = torch.tanh(self.att1(ht_u))
        alp = torch.tanh(self.att2(ha_u))
        al= self.bsftmax(alp)
        Tb = list(ht_u.shape)[1] 
        batch_size = list(ht_u.shape)[0]
        D = list(ht_u.shape)[2]
        u_vec = torch.bmm(al.view(batch_size, 1, Tb),ht_u.view(batch_size,Tb,D))
        u_vec = torch.squeeze(u_vec,0)
        
        lang_output = self.lang_classifier(u_vec)   
        
        return (lang_output, u1, u2)
       

def fix_data(X): 
    look_back1=20 
    look_back2=50
    Xdata1,Xdata2 = [],[]
    for i in range(0,len(X)-look_back1,1):
        a=X[i:(i+look_back1),:]        
        Xdata1.append(a.detach().cpu().numpy())
    #print('X',Xdata1)
    Xdata1=np.array(Xdata1)
    
    for i in range(0,len(X)-look_back2,2):  
        b=X[i:(i+look_back2):3,:]        
        Xdata2.append(b.detach().cpu().numpy())
    Xdata2=np.array(Xdata2)
    
    Xdata1 = torch.from_numpy(Xdata1).float()
    Xdata2 = torch.from_numpy(Xdata2).float()
    
    return(Xdata1, Xdata2)

if __name__ == "__main__":
    # se encoder
    modelencd = se_encoder(80, 80)
    modelencd#.cuda()
    
    # BiLSTMS
    model1 = LSTMNet()
    model2 = LSTMNet()
    
    model1.cuda()
    model2.cuda()
    
    # WSSL
    model = MSA_DAT_Net(model1,model2)
    model.cuda()
    
    #optimizer = optim.SGD(model.parameters(),lr = 0.01, momentum= 0.9)
    optimizer =  optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-5, betas=(0.9, 0.999), eps=1e-9)

    loss_lang = torch.nn.CrossEntropyLoss(reduction='mean')
    loss_wssl = torch.nn.CosineSimilarity()
    
    loss_lang.cuda()
    
    n_epoch = 30
    
    xtrain, ytrain = prepare_data()
    
    X = []
    
    l = len(xtrain)
    for e in range(n_epoch):
        cost = 0. 
        acc  = 0.
        # pass through SE, then WSSL
        for i in range(len(xtrain)):
            # passing through SE block
            temp = modelencd(xtrain[i])
            
            e1, e2 = fix_data(temp)
            
            fl,e1,e2 = model.forward(e1,e2)
            y_pred = torch.argmax(fl)

            # loss calculation
            err_l = loss_lang(fl,ytrain[i])
            err_wssl = abs(loss_wssl(e1,e2))               
            T_err = err_l + 0.25*err_wssl   

            T_err.backward()
            optimizer.step()
            cost = cost + T_err.item()
            acc += bool(ytrain[i]==y_pred)
        print("ZWSSL5:  epoch "+str(e+1)+" Loss= %.3f"%(cost/(i+1)),"Accuracy= "+str((acc/(i+1))))
        if e%5 == 0:
           torch.save(modelencd, './')
           torch.save(model, './')